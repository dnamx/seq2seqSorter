{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence recurrent neural network\n",
    "Here is a simple NN model that sorts strings of integer numbers.\n",
    "\n",
    "## Architecture\n",
    "The architecture is based on classic Sequence to Sequence with Attention [1].\n",
    "The implemented network features simple Elman recurrent network instead of LSTM in [1].\n",
    "\n",
    "<img src=\"seq2seq-model.svg\">\n",
    "\n",
    "The input integer numbers are encoded with binary vectors $x_i \\in \\{-1,1\\}^{N_b}$.\n",
    "The input sequence $x_i$ is fed to the recurrent encoder nodes $h_i$:\n",
    "$$ h_i = \\sigma(W_h x_i + U_h h_{i-1} + b_h), \\quad i = 1,\\ldots,N_{seq} $$\n",
    "here $h_0$ is model parameter.\n",
    "\n",
    "At each decoding step the encoder states $\\{h_i\\}$ are weighted and summed to context vector $c_i$:\n",
    "$$ c_i = \\sum_{j=1}^{N_{seq}} \\alpha_{ij} h_j, \\quad i = 1,\\ldots,N_{seq} $$\n",
    "The context $c_i$ is fed to recurrent decoder node $s_i$. The later takes the previous state $s_{i-1}$ along with last output element $\\hat{y}_{i-1}$:\n",
    "$$ s_i = \\sigma(W_s \\hat{y}_{i-1} + U_s s_{i-1} + C_s c_i + b_s), \\quad i = 1,\\ldots,N_{seq} $$\n",
    "again, the initial state $s_0$ is model parameter, $y_{sos}$ -- constant start-of-sequence marker. The output node $t_i$ takes the context $c_i$, state of decoder $s_i$ and last output element:\n",
    "$$ t_i = \\sigma(U_o s_i + V_o \\hat{y}_{i-1} + C_o c_i + b_o), \\quad i = 1,\\ldots,N_{seq} $$\n",
    "The real output $t_i$ is rounded to binary $\\hat{y}_i$\n",
    "$$ \\hat{y}_i = \\mathrm{sign} \\, t_i $$\n",
    "\n",
    "The context weighting function $\\alpha_{ij}$ is learned as well. At each decoding state $s_{i-1}$ an attention node $e_{ij}$ is calculated for each encoder state $h_j$:\n",
    "$$ e_{ij} = v_a^T \\sigma(W_a s_{s-1} + U_a h_j + b_a), \\quad j = 1,\\ldots,N_{seq} $$\n",
    "Then, the weights are normalized with soft-max:\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k}^{N_{seq}} \\exp(e_{ik})}, \\quad j = 1,\\ldots,N_{seq} $$\n",
    "\n",
    "## Training\n",
    "The cross-entropy of real output $t_i$ is chosen for loss function:\n",
    "$$ J = \\sum_{i=1}^{N_{seq}} \\sum_{j=1}^{N_b} \\log \\frac{y_{ij} t_{ij} + 1}{2} $$\n",
    "where $y_{i} \\in \\{-1,1\\}^{N_b}$ -- binary representation of reverence output\n",
    "\n",
    "In order to speed-up training, the decoder $s_i$ and output $t_i$ nodes are fed with reference vectors $y_i$\n",
    "$$ s_i = \\sigma(W_s y_{i-1} + U_s s_{i-1} + C_s c_i + b_s) $$\n",
    "$$ t_i = \\sigma(U_o s_i + V_o y_{i-1} + C_o c_i + b_o) $$\n",
    "\n",
    "\n",
    "[1] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" arXiv preprint arXiv:1409.0473 (2014)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
